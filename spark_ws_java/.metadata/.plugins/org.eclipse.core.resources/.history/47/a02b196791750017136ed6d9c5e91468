package sample.project.sparkJava;

import scala.Tuple2;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import java.util.Arrays;
import java.util.List;
import java.util.regex.Pattern;
import org.apache.spark.sql.SparkSession;

public class Sample {

	private static final Pattern SPACE = Pattern.compile(" ");

	public static void main(String[] args) {
		// TODO Auto-generated method stub

		if (args.length < 1) {
			System.err.println("Usage: JavaWordCount <file>");
			System.exit(1);
		}

		SparkSession spark = SparkSession.master("local").builder().appName("JavaWordCount")
				.getOrCreate();

		JavaRDD<String> lines = spark.read().textFile(args[0]).javaRDD();

		JavaRDD<String> words = lines.flatMap(s -> Arrays
				.asList(SPACE.split(s)).iterator());

		JavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(
				s, 1));

		JavaPairRDD<String, Integer> counts = ones.reduceByKey((i1, i2) -> i1
				+ i2);

		List<Tuple2<String, Integer>> output = counts.collect();
		for (Tuple2<?, ?> tuple : output) {
			System.out.println(tuple._1() + ": " + tuple._2());
		}
		spark.stop();

		/*
		 * String logFile =
		 * "/home/neil/Desktop/spark-2.2.0-bin-hadoop2.7/README.md"; SparkConf
		 * conf = new
		 * SparkConf().setAppName("Simple App").setMaster("local[*]");
		 * SparkContext sc = new SparkContext(conf);
		 * 
		 * RDD<String> textFile = sc.textFile(logFile,2); JavaPairRDD<String,
		 * Integer> counts = textFile .flatMap(s ->
		 * Arrays.asList(s.split(" ")).iterator()) .mapToPair(word -> new
		 * Tuple2<>(word, 1)) .reduceByKey((a, b) -> a + b);
		 * 
		 * counts.saveAsTextFile("/home/neil/Desktop/xyz");
		 */
	}
}
