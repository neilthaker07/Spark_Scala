package sample.project.sparkJava;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.easyrules.api.RulesEngine;
import org.easyrules.core.RulesEngineBuilder;
import org.json.simple.JSONObject;
import org.json.simple.parser.JSONParser;

import scala.Tuple2;

public class Demo {

	public static void main(String[] args) {
		// TODO Auto-generated method stub

		SparkSession spark = SparkSession.builder().master("local")
				.appName("Java Word Count")
				.config("spark.some.config.option", "some-value").getOrCreate();

		String inputFile = "/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/spark_ws_java/inputfile.json";
		JSONParser parser = new JSONParser();

		JavaRDD<Row> file = spark.read().json(inputFile).javaRDD();

		long log_id = (Long) jsonObject.get("LOG_ID");
		System.out.println(log_id);

		long temperature = (Long) jsonObject.get("TEMPERATURE");
		System.out.println(temperature);

		long pressure = (Long) jsonObject.get("PRESSURE");
		System.out.println(pressure);

		double gas = (Double) jsonObject.get("GAS");
		System.out.println(gas);

		JavaRDD<String> words = lines.flatMap(s -> Arrays
				.asList(SPACE.split(s)).iterator());

		JavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(
				s, 1));

		JavaPairRDD<String, Integer> counts = ones.reduceByKey((i1, i2) -> i1
				+ i2);

		/**
		 * Declare the rule
		 */
		HelloWorldRule helloWorldRule = new HelloWorldRule();

		/**
		 * Set business data to operate on
		 */

		helloWorldRule.setInput(input.trim());

		/**
		 * Create a rules engine and register the business rule
		 */
		RulesEngine rulesEngine = RulesEngineBuilder.aNewRulesEngine().build();

		rulesEngine.registerRule(helloWorldRule);

		/**
		 * Fire rules
		 */
		rulesEngine.fireRules();
	}

}
