package sample.project.sparkJava;

import java.util.Arrays;
import java.util.Scanner;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.SparkSession;
import org.easyrules.api.RulesEngine;
import org.easyrules.core.RulesEngineBuilder;

import scala.Tuple2;

public class Demo {

	public static void main(String[] args) {
		// TODO Auto-generated method stub
		
		String inputFile = "/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/spark_ws_java/inputfile.json";
		
		SparkSession spark = SparkSession.builder().master("local")
				.appName("Java Word Count")
				.config("spark.some.config.option", "some-value").getOrCreate();

		JavaRDD<String> lines = spark.read().textFile(inputFile).javaRDD();

		JavaRDD<String> words = lines.flatMap(s -> Arrays
				.asList(SPACE.split(s)).iterator());

		JavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(
				s, 1));

		JavaPairRDD<String, Integer> counts = ones.reduceByKey((i1, i2) -> i1
				+ i2);

		
		
		String input = scanner.nextLine();

		/**
		 * Declare the rule
		 */
		HelloWorldRule helloWorldRule = new HelloWorldRule();

		/**
		 * Set business data to operate on
		 */
		
		
		helloWorldRule.setInput(input.trim());

		/**
		 * Create a rules engine and register the business rule
		 */
		RulesEngine rulesEngine = RulesEngineBuilder.aNewRulesEngine().build();
		
		rulesEngine.registerRule(helloWorldRule);

		/**
		 * Fire rules
		 */
		rulesEngine.fireRules();
	}

}
