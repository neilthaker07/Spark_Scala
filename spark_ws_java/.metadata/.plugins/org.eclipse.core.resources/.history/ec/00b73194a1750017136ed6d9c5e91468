package sample.project.sparkJava;

import java.util.Arrays;

import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.easyrules.api.RulesEngine;
import org.easyrules.core.RulesEngineBuilder;

import scala.Tuple2;

public class Demo {

	public static void main(String[] args) {
		// TODO Auto-generated method stub

		SparkSession spark = SparkSession.builder().master("local")
				.appName("Rule Engine")
				.config("spark.some.config.option", "some-value").getOrCreate();

		String inputFile = "/home/neil/Neil_Work/MS_SJSU/scala_spark_learning/spark_ws_java/inputfile.json";

		JavaRDD<Row> lines = spark.read().json(inputFile).javaRDD();
		
/*		
		JavaRDD<Object> words = lines.flatMap(s -> Arrays
				.asList(SPACE.split(s)).iterator());

		JavaPairRDD<String, Integer> ones = words.mapToPair(s -> new Tuple2<>(
				s, 1));

		JavaPairRDD<String, Integer> counts = ones.reduceByKey((i1, i2) -> i1
				+ i2);
*/
		/**
		 * Declare the rule
		 */
		HelloWorldRule helloWorldRule = new HelloWorldRule();

		/**
		 * Set business data to operate on
		 */

		helloWorldRule.setInput(input.trim());

		/**
		 * Create a rules engine and register the business rule
		 */
		RulesEngine rulesEngine = RulesEngineBuilder.aNewRulesEngine().build();

		rulesEngine.registerRule(helloWorldRule);

		/**
		 * Fire rules
		 */
		rulesEngine.fireRules();
	}

}
